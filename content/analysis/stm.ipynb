{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive STM Workflow\n",
    "1. STM run with k=0\n",
    "1. NbClustering run over prelim STMs' theta to find better number of topics\n",
    "1. STM viewer webapp STM json data outputted\n",
    "1. Manual STM refinement / tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(magrittr)\n",
    "library(stm)\n",
    "library(jsonlite)\n",
    "library(doMC)\n",
    "library(foreach)\n",
    "library(NbClust)\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('../data_processing/tidy_questions.Rda')\n",
    "source('stmjson.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores to use on following computations\n",
    "registerDoMC(cores=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary STM\n",
    "Used to find baseline topic number using STM library methods (K=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STM does not produce meaningful clusters for these questions and are best removed.\n",
    "questions %>% names %>% as.data.frame %>% slice(4:6)\n",
    "questions <- questions[-c(4,5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start <- Sys.time()\n",
    "verbosity <- FALSE\n",
    "\n",
    "procs <- foreach(n = seq(length(questions))) %dopar% textProcessor(documents = questions[[n]][[1]],\n",
    "                                                                  metadata = questions[[n]][2],\n",
    "                                                                  customstopwords = c('art','arts'),\n",
    "                                                                  verbose = verbosity)\n",
    "\n",
    "docs <- foreach(n = seq(length(questions))) %dopar% prepDocuments(documents = procs[[n]]$documents, \n",
    "                                                                 vocab = procs[[n]]$vocab, meta = procs[[n]]$meta,\n",
    "                                                                 lower.thresh = ifelse(procs[[n]]$documents %>%\n",
    "                                                                                       length > 1000, 4, 3),\n",
    "                                                                 verbose = verbosity)\n",
    "\n",
    "prelim_stms <- foreach(n = seq(length(questions))) %dopar% stm(documents = docs[[n]]$documents,\n",
    "                                                               vocab = docs[[n]]$vocab, K = 0, \n",
    "                                                               data = docs[[n]]$meta, verbose = verbosity)\n",
    "\n",
    "time_taken <- Sys.time() - start\n",
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NbClust Cluster Analysis\n",
    "Used to find very close estimates of the best number of topics for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcas <- foreach(n = seq(length(prelim_stms))) %dopar% prcomp(x = (prelim_stms[[n]]$theta), scale. = T)\n",
    "\n",
    "nbcs <- foreach(n = seq(length(pcas))) %dopar% NbClust(data = select(data.frame(pca$x),\n",
    "                                                                     1:(stmobj$settings$dim$K - 5)),\n",
    "                                                       diss = daisy(pca$x),\n",
    "                                                       distance=NULL,\n",
    "                                                       min.nc=3,\n",
    "                                                       max.nc=27,\n",
    "                                                       method='complete',\n",
    "                                                       index='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The methods filtered out seem to always choose the lowest number of clusters considered every time.\n",
    "k_canidates <- c()\n",
    "for(i in seq(nbcs)) {\n",
    "    num_clust <- data.frame(method=nbcs[[i]]$Best.nc %>% t %>% rownames,\n",
    "               nc=nbcs[[i]]$Best.nc %>% t %>% as.data.frame() %>% pull(1)) %>% \n",
    "                    filter(method != 'Cindex' & method != 'DB' & method != 'Silhouette' &\n",
    "                           method != 'Duda' & method != 'PseudoT2' & method != 'Beale' &\n",
    "                           method != 'McClain' & method != 'Hubert' & method != 'Dindex')\n",
    "    num_clust %<>% pull(2) %>% table %>% data.frame %>% arrange(-Freq) %>% slice(1) %>% pull(1)\n",
    "    k_canidates %<>% c(num_clust)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved STMs\n",
    "Using NbClust recommended numbers of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start <- Sys.time()\n",
    "verbosity <- FALSE\n",
    "\n",
    "improved_stms <- foreach(n = seq(length(questions))) %dopar% stm(documents = docs[[n]]$documents,\n",
    "                                                                 vocab = docs[[n]]$vocab, K = k_canidates[n],\n",
    "                                                                 data = docs[[n]]$meta, verbose = verbosity)\n",
    "\n",
    "time_taken <- Sys.time() - start\n",
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting STM Data\n",
    "To be used with the webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_names <- c()\n",
    "for(i in seq(questions)) {\n",
    "    question_names %<>% c(names(questions[[i]][1]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './'\n",
    "\n",
    "foreach(n = seq(length(questions))) %dopar% create_json(\n",
    "    stm = improved_stms[[n]],\n",
    "    documents_raw = questions[[n]][question_names[n]] %>% slice(-procs[[n]]$docs.removed) %>% \n",
    "                                                                       slice(-docs[[n]]$docs.removed) %>% \n",
    "                                                                       pull,\n",
    "    documents_matrix = docs[[n]]$documents,\n",
    "    column_name = question_names[[n]],\n",
    "    title = names(questions[n]),\n",
    "    clustering_thresh = 1.4, #should be as low as possible without errors (raise in 0.1 steps if errors)\n",
    "    verbose = T,\n",
    "    directory = directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below for R converts the outputs of stm to json for use in tree viewer webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#\n",
    "# This is all code modified/extended from the stmCorrViz package (https://github.com/cran/stmCorrViz/tree/master/R).\n",
    "# Thank you Antonio Coppola, Margaret Roberts, Brandon Stewart, and Dustin Tingley.\n",
    "#\n",
    "###\n",
    "\n",
    "# requires questions, question_names\n",
    "find_cluster_docs <- function(topic_nums, raw_docs, question_num, min_total_prev = 0.3) {\n",
    "  responses_theta <- data.frame(raw_docs, thetas[[question_names[question_num]]][-1])\n",
    "  names(responses_theta) <- c('responses', names(responses_theta)[2:(ncol(responses_theta))])\n",
    "\n",
    "  min_prevalence <- min_total_prev/length(topic_nums)\n",
    "  responses_theta <- data.frame(responses_theta[1], responses_theta[-1] %>% select(topic_nums))\n",
    "\n",
    "  filter_string <- ''\n",
    "  for(i in seq(ncol(responses_theta)-1)) {\n",
    "    filter_string <- paste(filter_string, names(responses_theta)[i+1], '>', min_prevalence, '&')\n",
    "  }\n",
    "  filter_string <- filter_string %>% substr(2, nchar(filter_string)-2)\n",
    "\n",
    "  responses_sum <- data.frame(responses_theta,\n",
    "                              variance = responses_theta[-1] %>% apply(1, function(x)(diff(range(x)))) %>% as.vector,\n",
    "                              sum = responses_theta[-1] %>% apply(1, function(x)(sum(x))) %>% as.vector) %>%\n",
    "    filter_(filter_string) %>%\n",
    "    filter(sum >= max(min(length(topic_nums)/10, 0.9), 0.2)) %>%\n",
    "    filter(variance < 0.1) %>%\n",
    "    arrange(variance) %>%\n",
    "    select(1, sum, variance) %>%\n",
    "    slice(1:50)\n",
    "\n",
    "  responses_sum$responses %<>% as.character\n",
    "  return(responses_sum)\n",
    "}\n",
    "cluster_travel <- function(datajs, raw_docs, question_num) {\n",
    "  if(!is.null(datajs$children)) {\n",
    "    docs <- find_cluster_docs(as.numeric(datajs$topic_no), raw_docs, question_num)\n",
    "    datajs$thoughts <- docs$responses\n",
    "    datajs$thought_proportions <- docs$sum\n",
    "    datajs$thought_variances <- docs$variance\n",
    "    for(i in seq(datajs$children)) {\n",
    "      datajs$children[[i]] <- cluster_travel(datajs$children[[i]], raw_docs, question_num)\n",
    "    }\n",
    "  }\n",
    "  return(datajs)\n",
    "}\n",
    "create_json <- function(stm, documents_raw, documents_matrix, column_name,\n",
    "                        title='STM Tree', clustering_thresh=Inf, labels_number=7,\n",
    "                        verbose=F, instant=F, topic_labels=NULL, cluster_labels=NULL, directory=NULL,\n",
    "                        question_num=NULL)\n",
    "{\n",
    "  # with instant = True, the file is written to data.js\n",
    "  # with instant = False, the file is named <question>_<#ofTopics>_data.js\n",
    "  json <- stmJSON(mod = stm, documents_raw = documents_raw,\n",
    "                  documents_matrix = documents_matrix,\n",
    "                  topic_labels = topic_labels,\n",
    "                  cluster_labels = cluster_labels,\n",
    "                  title = title,\n",
    "                  clustering_threshold = clustering_thresh,\n",
    "                  labels_number = labels_number,\n",
    "                  verbose = verbose) %>%\n",
    "    .$json\n",
    "\n",
    "  name = paste(column_name, ncol(stm$theta),'data.js', sep='_')\n",
    "  if(instant){ name = 'data.js'}\n",
    "  if(!is.null(directory)){\n",
    "    if(substr(directory, nchar(directory), nchar(directory)) != '/') {\n",
    "      directory <- paste0(directory, '/')\n",
    "    }\n",
    "    name <- paste0(directory, name)\n",
    "  }\n",
    "  write(json, name, sep='')\n",
    "  if(!is.null(question_num)) {\n",
    "      json <- cluster_travel(read_json(name, simplifyVector=T, simplifyDataFrame = F,\n",
    "                                   simplifyMatrix = F), documents_raw, question_num)\n",
    "      json <- toJSON(json)\n",
    "  }\n",
    "  json <- paste0('var stm_data = ', json)\n",
    "  write(json, name, sep='')\n",
    "}\n",
    "\n",
    "stmJSON <-\n",
    "  function(mod, documents_raw=NULL, documents_matrix=NULL,\n",
    "           topic_labels=NULL, cluster_labels=NULL,\n",
    "           title=\"STM Model\",clustering_threshold=1.5,\n",
    "           labels_number=3, verbose=T){\n",
    "\n",
    "    # Generate baseline topic list\n",
    "    out <- list()\n",
    "\n",
    "    if(verbose==TRUE)\n",
    "      cat(\"Performing hierarchical topic clustering ... \\n\")\n",
    "\n",
    "    # Run hclust subroutine\n",
    "    clust <- clusterAnalysis(mod, labels_number, topic_labels)\n",
    "    if(clustering_threshold == Inf) {\n",
    "      clustering_threshold <- max(clust$height) - 0.1\n",
    "    }\n",
    "\n",
    "    if(verbose==TRUE)\n",
    "      cat(\"Generating JSON representation of the model ... \\n\")\n",
    "\n",
    "    # Extra data objects\n",
    "    thoughts <- stm::findThoughts(mod, documents_raw, n=50)\n",
    "    full_labels <- stm::labelTopics(mod)\n",
    "    topic_proportions <- colMeans(mod$theta)\n",
    "\n",
    "    # Find aggregation points\n",
    "    K <- mod$settings$dim$K\n",
    "    topic_to_topic_splits <- c()\n",
    "    for(i in seq(K-1))\n",
    "      if(clust$merge[i,1] <0 && clust$merge[i,2] < 0)\n",
    "        topic_to_topic_splits <- c(topic_to_topic_splits, i)\n",
    "    aggregate <- setdiff(which(clust$height <= clustering_threshold),\n",
    "                         topic_to_topic_splits)\n",
    "\n",
    "    # Produce merge list\n",
    "    merge_list <- list()\n",
    "    for(i in seq(K-1))\n",
    "      merge_list[[i]] <- clust$merge[i,]\n",
    "    names(merge_list) <- 1:(K-1)\n",
    "\n",
    "    # Collapse merge list\n",
    "    merge_list <- collapseMergeList(merge_list, clust$merge, aggregate, K)\n",
    "\n",
    "    # Build collapsed-clusters data structure\n",
    "    top_layer <- merge_list[paste(K-1)][[1]]\n",
    "    out$children <- buildClusters(list(), current = top_layer, merge_list,\n",
    "                                  labels=clust$labels, full_labels,\n",
    "                                  thoughts, topic_proportions, mod$theta)\n",
    "\n",
    "    # Implementation with diagnostics\n",
    "    #   out$children <- buildClusters(list(), current = top_layer, merge_list,\n",
    "    #                                 labels=clust$labels, full_labels,\n",
    "    #                                 thoughts, exclusivity_scores,\n",
    "    #                                 semcoh_scores)\n",
    "\n",
    "    # Get beta weights for model\n",
    "    beta_weights <- getBetaWeights(mod, documents_matrix)\n",
    "\n",
    "    # Assign cluster names\n",
    "    if(!is.null(cluster_labels)) {\n",
    "      sequence <- sapply(cluster_labels, function(x)(x$clustNum)) %>% as.vector\n",
    "      temp <- c()\n",
    "      for(i in seq(sequence)) {\n",
    "        temp <- c(temp, list(cluster_labels[[sequence[i]]]))\n",
    "      }\n",
    "      cluster_labels <- temp\n",
    "    }\n",
    "    out <- assignClusterNames(out, labels_number, beta_weights, mod$vocab, cluster_labels)\n",
    "\n",
    "    # Root Information\n",
    "    out$name <- title\n",
    "    out$this_root <- TRUE\n",
    "    out$summary <- utils::capture.output(mod)\n",
    "    out$proportions <- topic_proportions\n",
    "\n",
    "    # Convert structure to JSON\n",
    "    out_JSON <- jsonlite::toJSON(out, force=TRUE)\n",
    "    return(list(json=out_JSON, n_merge=length(merge_list)))\n",
    "  }\n",
    "\n",
    "clusterAnalysis <- function (stmobj, labels_number = 3, topic_labels=NULL)\n",
    "{\n",
    "  labels <- stm::labelTopics(stmobj)\n",
    "  theta <- stmobj$theta\n",
    "  d <- stats::dist(stats::cor(theta))\n",
    "  clust <- stats::hclust(d, method = \"complete\")\n",
    "  K <- stmobj$settings$dim$K\n",
    "  clust$labels <- rep(NA, K)\n",
    "  for (i in seq(K)) {\n",
    "    if(length(topic_labels[[i]]) > 0) {\n",
    "      clust$labels[i] <- topic_labels[[i]]$name\n",
    "    }\n",
    "    else {\n",
    "      l <- labels$frex[i, ]\n",
    "      l <- paste(l[1:labels_number], collapse = \", \")\n",
    "      clust$labels[i] <- l\n",
    "    }\n",
    "  }\n",
    "  return(clust)\n",
    "}\n",
    "\n",
    "collapseMergeList <-\n",
    "  function(merge_list, merge_matrix, aggregate, K){\n",
    "    calls <- which(merge_matrix %in% aggregate)\n",
    "\n",
    "    # Generate deletion sequence\n",
    "    old_refs <- c()\n",
    "    row_indices <- c()\n",
    "    col_indices <- c()\n",
    "    for(i in seq(length(calls))){\n",
    "      call <- calls[i]\n",
    "      if(call <= K-1){\n",
    "        row_indices[i] <- call\n",
    "        col_indices[i] <- 1\n",
    "      } else {\n",
    "        row_indices[i] <- call - K + 1\n",
    "        col_indices[i] <- 2\n",
    "      }\n",
    "      old_refs[i] <- merge_list[row_indices[i]][[1]][col_indices[i]]\n",
    "    }\n",
    "    deletion_seq <- data.frame(old_refs=old_refs,\n",
    "                               row_indices=row_indices,\n",
    "                               col_indices=col_indices)\n",
    "    deletion_seq <- deletion_seq[order(old_refs),]\n",
    "\n",
    "    # Perform collapsing: Insert new sequences\n",
    "    for(i in seq(nrow(deletion_seq))){\n",
    "      row_index <- deletion_seq$row_indices[i]\n",
    "      col_index <- deletion_seq$col_indices[i]\n",
    "      old_ref <- deletion_seq$old_refs[i]\n",
    "      #merge_list[row_index][[1]] <- merge_list[row_index][[1]][-col_index]\n",
    "      merge_list[row_index][[1]] <- c(merge_list[row_index][[1]],\n",
    "                                      merge_list[old_ref][[1]])\n",
    "    }\n",
    "\n",
    "    # Perform collapsing: Delete old references\n",
    "    for(row_index in names(merge_list)){\n",
    "      delete <- which(merge_list[paste(row_index)][[1]] %in% aggregate)\n",
    "      if(any(delete))\n",
    "        merge_list[row_index][[1]] <- merge_list[row_index][[1]][-delete]\n",
    "    }\n",
    "    merge_list <- merge_list[-aggregate]\n",
    "    return(merge_list)\n",
    "  }\n",
    "\n",
    "buildClusters <-\n",
    "  function(out, current, merge_list, labels, full_labels,\n",
    "           thoughts, topic_proportions, theta){\n",
    "\n",
    "    # Recursive definition\n",
    "    for(i in seq(length(current))){\n",
    "      out[[i]] <- list()\n",
    "      out[[i]]$name <- current[i]\n",
    "      if(current[i] > 0){\n",
    "        out[[i]]$children <- buildClusters(list(),\n",
    "                                           merge_list[paste(current[i])][[1]],\n",
    "                                           merge_list, labels=labels, full_labels,\n",
    "                                           thoughts, topic_proportions, theta)\n",
    "        out[[i]]$name <- current[i]\n",
    "      } else {\n",
    "        out[[i]]$size <- 1800 # if removed, code silently fails\n",
    "        out[[i]]$name <- labels[-current[i]]\n",
    "        out[[i]]$topic_no <- -current[i]\n",
    "        out[[i]]$thoughts <- c()\n",
    "        out[[i]]$thought_proportions <- c()\n",
    "        for(j in seq(50)) {\n",
    "          if(0.2 <= theta[thoughts$index[[-current[i]]][j], -current[i]]) {\n",
    "            out[[i]]$thoughts <- c(out[[i]]$thoughts, iconv(thoughts$docs[[-current[i]]][j], to='utf-8', sub=\"\"))\n",
    "            out[[i]]$thought_proportions <- c(out[[i]]$thought_proportions, theta[thoughts$index[[-current[i]]][j], -current[i]])\n",
    "          }\n",
    "          else {\n",
    "            # minimum of 10 documents\n",
    "            if(j >= 10) break\n",
    "            for(k in j:10) {\n",
    "              out[[i]]$thoughts <- c(out[[i]]$thoughts, iconv(thoughts$docs[[-current[i]]][k], to='utf-8', sub=\"\"))\n",
    "              out[[i]]$thought_proportions <- c(out[[i]]$thought_proportions, theta[thoughts$index[[-current[i]]][k], -current[i]])\n",
    "            }\n",
    "            break\n",
    "          }\n",
    "        }\n",
    "        out[[i]]$prob <- paste(full_labels$prob[-current[i],], collapse = \", \")\n",
    "        out[[i]]$frex <- paste(full_labels$frex[-current[i],], collapse = \", \")\n",
    "        out[[i]]$lift <- paste(full_labels$lift[-current[i],], collapse = \", \")\n",
    "        out[[i]]$score <- paste(full_labels$score[-current[i],], collapse = \", \")\n",
    "        out[[i]]$proportion <- format(round(topic_proportions[-current[i]], 2))\n",
    "      }\n",
    "    }\n",
    "    return(out)\n",
    "  }\n",
    "\n",
    "assignClusterNames <-\n",
    "  function(out, lab_no, beta_weights, vocab, cluster_labels=NULL){\n",
    "\n",
    "    members <- c()\n",
    "\n",
    "    for(i in seq(length(out$children))){\n",
    "      if (!('size' %in% names(out$children[[i]]))) # if child i is cluster\n",
    "        out$children[[i]] <- assignClusterNames(out$children[[i]], lab_no,\n",
    "                                                beta_weights, vocab, cluster_labels)\n",
    "    }\n",
    "\n",
    "    for(i in seq(length(out$children))){\n",
    "      members <- c(members, out$children[[i]]$topic_no)\n",
    "    }\n",
    "\n",
    "    out$topic_no <- members\n",
    "    margins <- marginalize(members, beta_weights$beta, beta_weights$weights)\n",
    "    labels <- vocab[margins$indices[1:lab_no]]\n",
    "\n",
    "    out$name <- paste(sample(labels, lab_no), collapse=\", \")\n",
    "    if(!is.null(cluster_labels)) {\n",
    "      for (j in seq(cluster_labels)) {\n",
    "        if(sum(members %in% cluster_labels[[j]]$topics) == length(members %in% cluster_labels[[j]]$topics) &\n",
    "           length(members) == length(cluster_labels[[j]]$topics)) {\n",
    "          if(!is.null(cluster_labels[[j]]$name[1]))\n",
    "            out$name <- cluster_labels[[j]]$name[1]\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return(out)\n",
    "  }\n",
    "\n",
    "getBetaWeights <-\n",
    "  function(model, documents=NULL) {\n",
    "    logbeta <- model$beta$logbeta\n",
    "    K <- model$settings$dim$K\n",
    "    vocab <- model$vocab\n",
    "\n",
    "    #Let's start by marginalizing\n",
    "    margbeta <- exp(logbeta[[1]])\n",
    "    if(length(logbeta) > 1) {\n",
    "      weights <- model$settings$covariates$betaindex\n",
    "      tab <- table(weights)\n",
    "      weights <- tab/sum(tab)\n",
    "      #marginalize\n",
    "      margbeta <- margbeta*weights[1]\n",
    "      for(i in 2:length(model$beta$logbeta)) {\n",
    "        margbeta <- margbeta + exp(model$beta$logbeta[[i]])*weights[i]\n",
    "      }\n",
    "    }\n",
    "\n",
    "    ##\n",
    "    # figure out how to weight the topics.\n",
    "    # NB: if they didn't provide topics use naive weights\n",
    "    #     otherwise calibrate thetas by the total counts\n",
    "    #     per document.\n",
    "    if(is.null(documents)) {\n",
    "      weights <- colSums(model$theta)\n",
    "    } else {\n",
    "      D.n <- unlist(lapply(documents, function(x) sum(x[2,])))\n",
    "      weights <- colSums(D.n*model$theta)\n",
    "    }\n",
    "\n",
    "    return(list(beta=margbeta, weights=weights))\n",
    "  }\n",
    "\n",
    "marginalize <-\n",
    "  function(members, beta, weights) {\n",
    "    w <- weights[members]/sum(weights[members])\n",
    "    pvec <- colSums(beta[members,,drop=FALSE]*w)\n",
    "    words <- order(pvec, decreasing=TRUE)\n",
    "    return(list(beta=pvec, indices=words))\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STM Refinement\n",
    "Use this space to change the number of topics, lower.thresh, and stopwords of questions to try to make a qualitatively better model after inspecting/comparing the model in the STM viewer webapp. A good place to start is looking at how well defined the \"no\"/\"none\" topic is (which appears in most of the questions asked)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#/ use this space to write down the question number, best number of topics, and custom stop words \n",
    "c(15, 9, )\n",
    "c(16, 11, c('art','arts','grow','growth','develop','development','way'))\n",
    "c(17, 13, c('art','arts','positive','negative','helped','major','really',\n",
    "              'much','made','think','dont','don\\'t','experience','experiences',\n",
    "              'college','most','life','role','provided')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i <- 19\n",
    "ntopics <- 11\n",
    "\n",
    "procs[[i]] <- textProcessor(documents = questions[[i]][[1]], \n",
    "              metadata = questions[[i]][2],\n",
    "              customstopwords = c('art','arts','grow','growth','develop','development','way'))\n",
    "#               customstopwords = c('art','arts','positive','negative','helped','major','really',\n",
    "#                                   'much','made','think','dont','don\\'t','experience','experiences',\n",
    "#                                   'college','most','life','role','provided'))\n",
    "#               customstopwords = c('art','arts'))\n",
    "\n",
    "docs[[i]] <- prepDocuments(documents = procs[[i]]$documents,\n",
    "              vocab = procs[[i]]$vocab,\n",
    "              meta = procs[[i]]$meta,\n",
    "              lower.thresh = 3)\n",
    "              #lower.thresh = ifelse(procs[[i]]$documents %>% length > 1000, 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start <- Sys.time()\n",
    "stmobj <- stm(documents = docs[[i]]$documents,\n",
    "                vocab = docs[[i]]$vocab,\n",
    "                K = ntopics,\n",
    "                data=docs[[i]]$meta,\n",
    "                verbose=F)\n",
    "print(Sys.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out one of the following lines\n",
    "labels <- read_json('labels/sr_othergrowth_labels.json')\n",
    "# labels <- NULL\n",
    "\n",
    "if (is.null(labels)) {\n",
    "    labels$topics <- NULL\n",
    "    labels$clusters <- NULL\n",
    "}\n",
    "\n",
    "create_json(\n",
    "    stm = stmobj,\n",
    "    documents_raw = questions[[i]][question_names[i]] %>% slice(-procs[[i]]$docs.removed) %>% \n",
    "                                                                       slice(-docs[[i]]$docs.removed) %>% \n",
    "                                                                       pull,\n",
    "    documents_matrix = docs[[i]]$documents,\n",
    "    column_name = question_names[[i]],\n",
    "    title = names(questions[i]),\n",
    "    clustering_thresh = 1.4, # should be as low as possible w/o errors\n",
    "    instant = T, # names the json data.json if set to true\n",
    "    topic_labels = labels$topics,\n",
    "    cluster_labels = labels$clusters,\n",
    "    directory = './'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
